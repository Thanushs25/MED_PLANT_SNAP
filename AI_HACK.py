# -*- coding: utf-8 -*-
"""AI HACK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10gMcn4WhbtLHum5faXcujLtnOmSzNELW
"""

from google.colab import drive
drive.mount('/content/gdrive')

import numpy as np
import os
from sklearn.metrics import confusion_matrix
import seaborn as sn; sn.set(font_scale=1.4)
from sklearn.utils import shuffle           
import matplotlib.pyplot as plt             
import cv2                                 
import tensorflow as tf
import PIL.ImageOps as ImageOps
import pandas as pd
import PIL.Image as Image
from tqdm import tqdm
import tensorflow_datasets as tfds
from tensorflow.keras import layers

class_names = ["Arive-Dantu", "Basale", "Betel", "Crape_Jasmine", "Curry", "Drumstick", "Fenugreek", "Guava", "Hibiscus", "Indian_Beech",
               "Indian_Mustard", "Jackfruit", "Jamaica_Cherry-Gasagase", "Jamun", "Jasmine", "Karanda", "Lemon",
               "Mango", "Mexican_Mint", "Mint", "Neem", "Oleander", "Parijata", "Peepal", "Pomegranate", "Rasna", "Rose_apple",
               "Roxburgh_fig", "Sandalwood", "Tulsi"]
class_names_label = {class_name: i for i, class_name in enumerate(class_names)}

nb_classes = len(class_names)

IMAGE_SIZE = (150, 150)

def pre_process(img_path):
    image = cv2.imread(img_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = cv2.resize(image, IMAGE_SIZE) 
    return image

def load_data():
    
    datasets = ["/content/gdrive/MyDrive/AI HACK/PLANT/TRAIN","/content/gdrive/MyDrive/AI HACK/PLANT/TEST"]
    output = []
    
    # Iterate through training and test sets
    for dataset in datasets:
        
        images = []
        labels = []
        
        print("Loading {}".format(dataset))
        
        # Iterate through each folder corresponding to a category
        for folder in os.listdir(dataset):
            label = class_names_label[folder]
            
            # Iterate through each image in our folder
            for file in tqdm(os.listdir(os.path.join(dataset, folder))):
                
                # Get the path name of the image
                img_path = os.path.join(os.path.join(dataset, folder), file)
                
                # Open and resize the img
                image = pre_process(img_path) 
                
                # Append the image and its corresponding label to the output
                images.append(image)
                labels.append(label)
                
        images = np.array(images, dtype = 'float32')
        labels = np.array(labels, dtype = 'int32')   
        
        output.append((images, labels))

    return output

(train_images, train_labels), (test_images, test_labels) = load_data()

train_images, train_labels = shuffle(train_images, train_labels, random_state=25)

n_train = train_labels.shape[0]
n_test = test_labels.shape[0]

print ("Number of training examples: {}".format(n_train))
print ("Number of testing examples: {}".format(n_test))
print ("Each image is of size: {}".format(IMAGE_SIZE))

train_images = train_images / 255.0 
test_images = test_images / 255.0

def display_examples(class_names, images, labels):
    
    fig = plt.figure(figsize=(15,15))
    fig.suptitle("EXAMPLES OF IMAGES FROM THE DATASET", fontsize=16)
    for i in range(16):
        plt.subplot(4,4,i+1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(images[i], cmap=plt.cm.binary)
        plt.xlabel(class_names[labels[i]],fontsize=10)
    plt.show()

def display_random_image(class_names, images, labels):
    
    index = np.random.randint(images.shape[0])
    plt.figure()
    plt.imshow(images[index])
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.title('Image #{} : '.format(index) + class_names[labels[index]])
    plt.show()

display_examples(class_names, train_images, train_labels)

model = tf.keras.Sequential([
  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(30, activation='softmax')
])

model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])

history = model.fit(train_images, train_labels, batch_size=128, epochs=20, validation_split = 0.2)

def plot_accuracy_loss(history):
    """
        We plot the accuracy and the loss during the training of the nn.
    """
    fig = plt.figure(figsize=(10,5))

    # Plot accuracy
    plt.subplot(221)
    plt.plot(history.history['accuracy'],'bo--', label = "accuracy")
    plt.plot(history.history['val_accuracy'], 'ro--', label = "val_accuracy")
    plt.title("train_acc vs val_acc")
    plt.ylabel("accuracy")
    plt.xlabel("epochs")
    plt.legend()

    # Plot loss function
    plt.subplot(222)
    plt.plot(history.history['loss'],'bo--', label = "loss")
    plt.plot(history.history['val_loss'], 'ro--', label = "val_loss")
    plt.title("train_loss vs val_loss")
    plt.ylabel("loss")
    plt.xlabel("epochs")

    plt.legend()
    plt.show()

plot_accuracy_loss(history)

test_loss = model.evaluate(test_images, test_labels)

predictions = model.predict(test_images)     # Vector of probabilities
pred_labels = np.argmax(predictions, axis = 1) # We take the highest probability

display_random_image(class_names, test_images, pred_labels)

data = np.ndarray(shape=(1, 150, 150, 3), dtype=np.float32)
image = Image.open('/content/gdrive/MyDrive/AI HACK/PLANT/TEST/Jackfruit/AH-S-020.jpg')
#image sizing
size = (150, 150)
image = ImageOps.fit(image, size, Image.ANTIALIAS)

#turn the image into a numpy array
image_array = np.asarray(image)
# Normalize the image
normalized_image_array = (image_array.astype(np.float32) / 127.0) - 1

# Load the image into the array
data[0] = normalized_image_array

# run the inference
prediction = model.predict(data)
    

pred_label = np.argmax(prediction, axis = 1) # We take the highest probability
class_prediction = class_names[pred_label[0]]
class_prediction

df=pd.read_csv('/content/gdrive/MyDrive/AI HACK/details.csv')
df

df.iloc[29,0]

def answer(class_prediction):
  for i in range (0,29):
    if class_prediction==df.iloc[i,0]:
      print("The plant that you have uploaded is : ",df.iloc[i,0])
      print("Scientific Name : ",df.iloc[i,1])
      print("Disease Treated : ",df.iloc[i,2])
      print("Preparation Method : ",df.iloc[i,3])
      print("Administration : ",df.iloc[i,4])

answer(class_prediction)

model.save('MED_PLANT_SNAP(19.03.2023).h5')